{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping for Reddit Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to extract the following features from Reddit posts collected in December, 2021, and early January, 2022, to identify which characteristics of a Reddit post are associated with a high level of engagement in a thread as measured by the number of comments on the thread. The features extracted from each post include:\n",
    "\n",
    "* Title of the thread\n",
    "* Time since the thread was posted\n",
    "* Number of comments\n",
    "* Subreddit\n",
    "* An indicator for whether a post contains a video\n",
    "* An indicator for whether a post contains an image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries and set up a webdriver to scrape the data using selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T23:56:58.877917Z",
     "start_time": "2022-05-10T23:56:57.239546Z"
    }
   },
   "outputs": [],
   "source": [
    "import schedule\n",
    "from datetime import datetime, timedelta, time\n",
    "from time import sleep\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T23:57:03.393939Z",
     "start_time": "2022-05-10T23:56:58.880944Z"
    }
   },
   "outputs": [],
   "source": [
    "ser = Service('chromedriver/chromedriver')\n",
    "op = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=ser, options=op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T23:57:03.401099Z",
     "start_time": "2022-05-10T23:56:57.241Z"
    }
   },
   "outputs": [],
   "source": [
    "URL = \"http://www.reddit.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T23:57:03.403867Z",
     "start_time": "2022-05-10T23:56:57.242Z"
    }
   },
   "outputs": [],
   "source": [
    "driver.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T23:57:03.407190Z",
     "start_time": "2022-05-10T23:56:57.244Z"
    }
   },
   "outputs": [],
   "source": [
    "def web_scrapping():\n",
    "    \"\"\" This function is used to scroll to the bottom of the Reddit page opened by the web-driver,\n",
    "        creating a BeautifulSoup object containing the HTML of the scrapped pages, accessing the tags\n",
    "        and classes necessary to extract a thread's title, time since it was posted, number of comments,\n",
    "        subreddit, an indicator for whether the thread contained an image, and an indicator for whether a thread\n",
    "        contained a video. As a next step, the data are combined to a dataframe and exported to a CSV file for use\n",
    "        in the analysis.\n",
    "    \"\"\"\n",
    "    for i in range(1,301):\n",
    "        if i % 50 == 0:\n",
    "            print(\"sleep repetition:\", i)\n",
    "        sleep(3)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    print(driver.title)\n",
    "    assert 'Reddit' in driver.title\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "    title = []\n",
    "    for entry in soup.findAll('div', {'class':'_1poyrkZ7g36PawDueRza-J'}):\n",
    "        try:\n",
    "            title.append(entry.find('h3').text)\n",
    "        except:\n",
    "            title.append('No value')\n",
    "\n",
    "    timestamps = []\n",
    "    for entry in soup.findAll('div', {'class':'_1poyrkZ7g36PawDueRza-J'}):\n",
    "        try:\n",
    "            timestamps.append(entry.find('a', {'class':'_3jOxDPIQ0KaOWpzvSQo-1s'}).text)\n",
    "        except:\n",
    "            timestamps.append('No value')\n",
    "    \n",
    "    comments = []\n",
    "    for entry in soup.findAll('div', {'class':'_1poyrkZ7g36PawDueRza-J'}):\n",
    "        try:\n",
    "            comments.append(entry.find('span', {'class':'FHCV02u6Cp2zYL0fhQPsO'}).text)\n",
    "        except:\n",
    "            comments.append('No value')\n",
    "    \n",
    "    subreddit = []\n",
    "    for entry in soup.findAll('div', {'class':'_1poyrkZ7g36PawDueRza-J'}):\n",
    "        try:\n",
    "            subreddit.append(entry.find('div', {'class':'_2mHuuvyV9doV3zwbZPtIPG'}).text)\n",
    "        except:\n",
    "            subreddit.append('No value')\n",
    "            \n",
    "    image = []\n",
    "    for entry in soup.findAll('div', {'class':'_1poyrkZ7g36PawDueRza-J'}):\n",
    "        try:\n",
    "            image.append(str(entry.find('img', {'class': '_2_tDEnGMLxpM6uOa2kaDB3'})).split()[0])\n",
    "        except:\n",
    "            image.append('No value')\n",
    "            \n",
    "    video = []\n",
    "    for entry in soup.findAll('div', {'class':'_1poyrkZ7g36PawDueRza-J'}):\n",
    "        try:\n",
    "            video.append(str(entry.find('video', {'class': '_1EQJpXY7ExS04odI1YBBlj'})).split()[0])\n",
    "        except:\n",
    "            video.append('No value')\n",
    "    \n",
    "    assert len(title) == len(timestamps) == len(comments) == len(subreddit) == len(image) == len(video)\n",
    "    \n",
    "    df = pd.DataFrame({'title':title,'time':timestamps,'subreddit':subreddit,'number_comments':comments,\n",
    "                  'image':image, 'video':video})\n",
    "    \n",
    "    df['image'] = df['image'].apply(lambda x: 1 if x == '<img' else 0)\n",
    "    df['video'] = df['video'].apply(lambda x: 1 if x == '<video' else 0)\n",
    "    \n",
    "    df = df.drop_duplicates()\n",
    "    return df.to_csv('./data/web-scrapping-{}.csv'.format(pd.datetime.now().strftime(\"%Y-%m-%d %H%M%S\")),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the web-scrapping function for 5-hour intervals.\n",
    "[This documentation was helpful in automating the execution of the web-scrapping function](https://schedule.readthedocs.io/en/stable/examples.html). [This](https://www.youtube.com/watch?v=zwIGxcDxS5o) YouTube video was helpful as well. [This](https://www.programiz.com/python-programming/datetime/current-time) resource was helpful in setting up the datetime object used below to allow the while loop to run until a predetermined time. [This](http://selenium-python.readthedocs.io/faq.html) resource was helpful in understanding how to scroll down to the bottom of the page. [This](https://stackoverflow.com/questions/67505537/add-timestamp-to-file-name-during-saving-data-frame-in-csv) resource was helpful in finding a way to add a timestamp to each CSV file generated from the extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T23:57:03.428261Z",
     "start_time": "2022-05-10T23:56:57.245Z"
    }
   },
   "outputs": [],
   "source": [
    "schedule.every(5).hours.until(\"2022-01-03 23:59\").do(web_scrapping)\n",
    "while datetime.now().strftime(\"%H:%M:%S\") < \"23:59:00\":\n",
    "    schedule.run_pending()\n",
    "    sleep(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a file of scrapped data for use in the analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the timestamp of when the data was scrapped to estimate the time when posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T23:57:03.432017Z",
     "start_time": "2022-05-10T23:56:57.247Z"
    }
   },
   "outputs": [],
   "source": [
    "for file in sorted(os.listdir('data'),reverse=True):\n",
    "    df = pd.read_csv(f'./data/{file}')\n",
    "    df['date_scrapped'] = ''.join(''.join(file.split('web-scrapping-')).split('.csv'))\n",
    "    df.to_csv(f'./data/{file}', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test that the date when the data was scrapped was added correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T23:57:03.436336Z",
     "start_time": "2022-05-10T23:56:57.248Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.read_csv('./data/web-scrapping-2021-12-28 063123.csv').head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Append all scrapped data into one data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T23:57:03.442991Z",
     "start_time": "2022-05-10T23:56:57.250Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv('./data/'+file) for file in sorted(os.listdir('data'),reverse=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns that will not be used in the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T23:57:03.445380Z",
     "start_time": "2022-05-10T23:56:57.251Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0', 'timestamps'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T23:57:03.447933Z",
     "start_time": "2022-05-10T23:56:57.255Z"
    }
   },
   "outputs": [],
   "source": [
    "df.sort_values(['title','date_scrapped']).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T23:57:03.450332Z",
     "start_time": "2022-05-10T23:56:57.257Z"
    }
   },
   "outputs": [],
   "source": [
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a datetime variable to estimate the time when the thread was posted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T23:57:03.455908Z",
     "start_time": "2022-05-10T23:56:57.258Z"
    }
   },
   "outputs": [],
   "source": [
    "for i,date in enumerate(df['date_scrapped']):\n",
    "    df.loc[i,'date_scrapped'] = datetime.strptime(date,'%Y-%m-%d %H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T23:57:03.459459Z",
     "start_time": "2022-05-10T23:56:57.260Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop posts without a number of comments or a time value or the image and video indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T23:57:03.464686Z",
     "start_time": "2022-05-10T23:56:57.261Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[(df['number_comments'] != '0 comments') & (df['number_comments'] != 'No value')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T23:57:03.469742Z",
     "start_time": "2022-05-10T23:56:57.262Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df['time'] != 'No value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T23:57:03.472376Z",
     "start_time": "2022-05-10T23:56:57.264Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[(~df['image'].isna()) & (~df['video'].isna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reset the index and export the file that will be used as input data for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T23:57:03.476627Z",
     "start_time": "2022-05-10T23:56:57.270Z"
    }
   },
   "outputs": [],
   "source": [
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T23:57:03.479452Z",
     "start_time": "2022-05-10T23:56:57.271Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T23:57:03.487380Z",
     "start_time": "2022-05-10T23:56:57.272Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('./data/web_scrapping_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
